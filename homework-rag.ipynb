{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27005ecc",
   "metadata": {},
   "source": [
    "Implementing RAG based on my HW \n",
    "\n",
    "im just tryna learn rag. This project is an implementation based off of my COGSCI-132 hw. \n",
    "\n",
    "RAG can be broken down into 5 actionable steps:\n",
    "\n",
    "1. Load: We load in our data using DocumentLoaders. \n",
    "2. Split: Our data is split into chunks in order to be captured within the context window. We will use RecursiveTextSplitter to recursively split our text until the chunks are within the limt \n",
    "3. Store: The split data is then converted into embeddings. Embeddings is a way for text to be represented numerically for computers to understand. These embeddings are then stored in a vector store.\n",
    "4. Retrieve: The most relevant embeddings are retrieved based on a similarity search with the query.\n",
    "5. Generate: Both the query and most relevant context are then passed into the LLM to output the most accurate response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb4ec5e",
   "metadata": {},
   "source": [
    "Setup\n",
    "\n",
    "We instantiate LangSmith for tracing, as well as OpenAI. The import getpass is used for a more secure input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb919318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# our text splitters and langchain\n",
    "%pip install --quiet --upgrade langchain-text-splitters langchain_community langgraph \n",
    "\n",
    "# our llm \n",
    "%pip install -qU \"langchain[openai]\"\n",
    "\n",
    "# our vector db \n",
    "%pip install -qU langchain-chroma "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "906faf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass \n",
    "import os \n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"True\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa830b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass('Enter OpenAI API Key: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63beb884",
   "metadata": {},
   "source": [
    "Here we will instantiate our LLM model, vector DB and our embeddings function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ec569e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma \n",
    "\n",
    "llm = init_chat_model(\"openai:gpt-5-nano\") # or (model=\"gpt-5-nano\", model_provider=\"openai\")\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model='text-embeddings-3-large')\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"homework\",\n",
    "    embedding_function=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e64e7df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
