{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27005ecc",
   "metadata": {},
   "source": [
    "Implementing RAG based on my HW \n",
    "\n",
    "im just tryna learn rag. This project is an implementation based off of my COGSCI-132 hw. \n",
    "\n",
    "RAG can be broken down into 5 actionable steps:\n",
    "\n",
    "1. Load: We load in our data using DocumentLoaders. \n",
    "2. Split: Our data is split into chunks in order to be captured within the context window. We will use RecursiveTextSplitter to recursively split our text until the chunks are within the limt \n",
    "3. Store: The split data is then converted into embeddings. Embeddings is a way for text to be represented numerically for computers to understand. These embeddings are then stored in a vector store.\n",
    "4. Retrieve: The most relevant embeddings are retrieved based on a similarity search with the query.\n",
    "5. Generate: Both the query and most relevant context are then passed into the LLM to output the most accurate response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb4ec5e",
   "metadata": {},
   "source": [
    "Setup\n",
    "\n",
    "We instantiate LangSmith for tracing, as well as OpenAI. The import getpass is used for a more secure input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb919318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: langchain-unstructured[local] in ./.venv/lib/python3.13/site-packages (0.1.5)\n",
      "Requirement already satisfied: langchain-core<0.4,>=0.3 in ./.venv/lib/python3.13/site-packages (from langchain-unstructured[local]) (0.3.76)\n",
      "Requirement already satisfied: unstructured-client<0.26.0,>=0.25.0 in ./.venv/lib/python3.13/site-packages (from langchain-unstructured[local]) (0.25.9)\n",
      "Requirement already satisfied: langsmith>=0.3.45 in ./.venv/lib/python3.13/site-packages (from langchain-core<0.4,>=0.3->langchain-unstructured[local]) (0.4.28)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in ./.venv/lib/python3.13/site-packages (from langchain-core<0.4,>=0.3->langchain-unstructured[local]) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.venv/lib/python3.13/site-packages (from langchain-core<0.4,>=0.3->langchain-unstructured[local]) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.venv/lib/python3.13/site-packages (from langchain-core<0.4,>=0.3->langchain-unstructured[local]) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in ./.venv/lib/python3.13/site-packages (from langchain-core<0.4,>=0.3->langchain-unstructured[local]) (4.15.0)\n",
      "Requirement already satisfied: packaging>=23.2 in ./.venv/lib/python3.13/site-packages (from langchain-core<0.4,>=0.3->langchain-unstructured[local]) (25.0)\n",
      "Requirement already satisfied: pydantic>=2.7.4 in ./.venv/lib/python3.13/site-packages (from langchain-core<0.4,>=0.3->langchain-unstructured[local]) (2.11.9)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.venv/lib/python3.13/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.3->langchain-unstructured[local]) (3.0.0)\n",
      "Requirement already satisfied: certifi>=2023.7.22 in ./.venv/lib/python3.13/site-packages (from unstructured-client<0.26.0,>=0.25.0->langchain-unstructured[local]) (2025.8.3)\n",
      "Requirement already satisfied: charset-normalizer>=3.2.0 in ./.venv/lib/python3.13/site-packages (from unstructured-client<0.26.0,>=0.25.0->langchain-unstructured[local]) (3.4.3)\n",
      "Requirement already satisfied: cryptography>=3.1 in ./.venv/lib/python3.13/site-packages (from unstructured-client<0.26.0,>=0.25.0->langchain-unstructured[local]) (45.0.7)\n",
      "Requirement already satisfied: dataclasses-json>=0.6.4 in ./.venv/lib/python3.13/site-packages (from unstructured-client<0.26.0,>=0.25.0->langchain-unstructured[local]) (0.6.7)\n",
      "Requirement already satisfied: deepdiff>=6.0 in ./.venv/lib/python3.13/site-packages (from unstructured-client<0.26.0,>=0.25.0->langchain-unstructured[local]) (8.6.1)\n",
      "Requirement already satisfied: httpx>=0.27.0 in ./.venv/lib/python3.13/site-packages (from unstructured-client<0.26.0,>=0.25.0->langchain-unstructured[local]) (0.28.1)\n",
      "Requirement already satisfied: idna>=3.4 in ./.venv/lib/python3.13/site-packages (from unstructured-client<0.26.0,>=0.25.0->langchain-unstructured[local]) (3.10)\n",
      "Requirement already satisfied: jsonpath-python>=1.0.6 in ./.venv/lib/python3.13/site-packages (from unstructured-client<0.26.0,>=0.25.0->langchain-unstructured[local]) (1.0.6)\n",
      "Requirement already satisfied: marshmallow>=3.19.0 in ./.venv/lib/python3.13/site-packages (from unstructured-client<0.26.0,>=0.25.0->langchain-unstructured[local]) (3.26.1)\n",
      "Requirement already satisfied: mypy-extensions>=1.0.0 in ./.venv/lib/python3.13/site-packages (from unstructured-client<0.26.0,>=0.25.0->langchain-unstructured[local]) (1.1.0)\n",
      "Requirement already satisfied: nest-asyncio>=1.6.0 in ./.venv/lib/python3.13/site-packages (from unstructured-client<0.26.0,>=0.25.0->langchain-unstructured[local]) (1.6.0)\n",
      "Requirement already satisfied: pypdf>=4.0 in ./.venv/lib/python3.13/site-packages (from unstructured-client<0.26.0,>=0.25.0->langchain-unstructured[local]) (6.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.13/site-packages (from unstructured-client<0.26.0,>=0.25.0->langchain-unstructured[local]) (2.9.0.post0)\n",
      "Requirement already satisfied: requests>=2.31.0 in ./.venv/lib/python3.13/site-packages (from unstructured-client<0.26.0,>=0.25.0->langchain-unstructured[local]) (2.32.5)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in ./.venv/lib/python3.13/site-packages (from unstructured-client<0.26.0,>=0.25.0->langchain-unstructured[local]) (1.0.0)\n",
      "Requirement already satisfied: six>=1.16.0 in ./.venv/lib/python3.13/site-packages (from unstructured-client<0.26.0,>=0.25.0->langchain-unstructured[local]) (1.17.0)\n",
      "Requirement already satisfied: typing-inspect>=0.9.0 in ./.venv/lib/python3.13/site-packages (from unstructured-client<0.26.0,>=0.25.0->langchain-unstructured[local]) (0.9.0)\n",
      "Requirement already satisfied: urllib3>=1.26.18 in ./.venv/lib/python3.13/site-packages (from unstructured-client<0.26.0,>=0.25.0->langchain-unstructured[local]) (2.5.0)\n",
      "Requirement already satisfied: cffi>=1.14 in ./.venv/lib/python3.13/site-packages (from cryptography>=3.1->unstructured-client<0.26.0,>=0.25.0->langchain-unstructured[local]) (2.0.0)\n",
      "Requirement already satisfied: pycparser in ./.venv/lib/python3.13/site-packages (from cffi>=1.14->cryptography>=3.1->unstructured-client<0.26.0,>=0.25.0->langchain-unstructured[local]) (2.23)\n",
      "Requirement already satisfied: orderly-set<6,>=5.4.1 in ./.venv/lib/python3.13/site-packages (from deepdiff>=6.0->unstructured-client<0.26.0,>=0.25.0->langchain-unstructured[local]) (5.5.0)\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.13/site-packages (from httpx>=0.27.0->unstructured-client<0.26.0,>=0.25.0->langchain-unstructured[local]) (4.10.0)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.13/site-packages (from httpx>=0.27.0->unstructured-client<0.26.0,>=0.25.0->langchain-unstructured[local]) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.13/site-packages (from httpcore==1.*->httpx>=0.27.0->unstructured-client<0.26.0,>=0.25.0->langchain-unstructured[local]) (0.16.0)\n",
      "Requirement already satisfied: orjson>=3.9.14 in ./.venv/lib/python3.13/site-packages (from langsmith>=0.3.45->langchain-core<0.4,>=0.3->langchain-unstructured[local]) (3.11.3)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in ./.venv/lib/python3.13/site-packages (from langsmith>=0.3.45->langchain-core<0.4,>=0.3->langchain-unstructured[local]) (0.25.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.13/site-packages (from pydantic>=2.7.4->langchain-core<0.4,>=0.3->langchain-unstructured[local]) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./.venv/lib/python3.13/site-packages (from pydantic>=2.7.4->langchain-core<0.4,>=0.3->langchain-unstructured[local]) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./.venv/lib/python3.13/site-packages (from pydantic>=2.7.4->langchain-core<0.4,>=0.3->langchain-unstructured[local]) (0.4.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./.venv/lib/python3.13/site-packages (from anyio->httpx>=0.27.0->unstructured-client<0.26.0,>=0.25.0->langchain-unstructured[local]) (1.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# our text splitters and langchain\n",
    "%pip install --quiet --upgrade langchain-text-splitters langchain_community langgraph \n",
    "\n",
    "# our llm \n",
    "%pip install -qU \"langchain[openai]\"\n",
    "\n",
    "# our vector db \n",
    "%pip install -qU langchain-chroma \n",
    "\n",
    "# our pdf document loader\n",
    "%pip install \"langchain-unstructured[local]\" \n",
    "%pip install --upgrade --quiet langchain-unstructured unstructured-client unstructured \"unstructured[pdf]\" python-magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "906faf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass \n",
    "import os \n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"True\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa830b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass('Enter OpenAI API Key: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63beb884",
   "metadata": {},
   "source": [
    "Here we will instantiate our LLM model, vector DB and our embeddings function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2ec569e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma \n",
    "\n",
    "llm = init_chat_model(\"openai:gpt-5-nano\") # or (model=\"gpt-5-nano\", model_provider=\"openai\")\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model='text-embedding-3-large')\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"homework\",\n",
    "    embedding_function=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e64e7df",
   "metadata": {},
   "source": [
    "Indexing \n",
    "\n",
    "We are going to index our data using **Unstructured**. This will allow us to parse not only text but also images within PDF documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "586fe680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n",
      "Number of documents:  796\n",
      "Length of characteres:  58532\n"
     ]
    }
   ],
   "source": [
    "from langchain_unstructured import UnstructuredLoader\n",
    "from unstructured.cleaners.core import clean_extra_whitespace\n",
    "import json\n",
    "\n",
    "file_path = 'data/ermentrout-and-terman-ch-1.pdf'\n",
    "\n",
    "loader = UnstructuredLoader(\n",
    "    file_path,\n",
    "    post_processors=[clean_extra_whitespace]\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# chroma doesnt accept complex metadata like dicts or lists\n",
    "# this code loops through all docs, all key value pairs inside the metadata and checks instances of dicts or lists\n",
    "# it will replace the value with a stringified version of the original value.\n",
    "for doc in docs:\n",
    "    for key,value in doc.metadata.items():\n",
    "        if isinstance(value, (dict,list)):\n",
    "            doc.metadata[key] = json.dumps(value)\n",
    "\n",
    "\n",
    "total_chars = \"\".join([doc.page_content for doc in docs])\n",
    "print(\"Number of documents: \", len(docs))\n",
    "print(\"Length of characteres: \", len(total_chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24ac2c8",
   "metadata": {},
   "source": [
    "Splitting Documents\n",
    "\n",
    "The length of characters exceeds the context window, and it would be difficult for models to find information with very long inptus.\n",
    "\n",
    "To handle this we split the documents into chunks for embedding and vector storage to retrieve only the most relevant information. \n",
    "\n",
    "We will use RecursiveTextSplitter to recursively split our text until the chunks are within the context window limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7831e962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split documents into 797\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Initialize RecursiveCharacterTextSplitter \n",
    "# params: chunks, overlap, add_start_index \n",
    "# overlap for the context to not be cutoff \n",
    "# add_start_index makes each chunk store their orginal position in the text \n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1200,\n",
    "    chunk_overlap=300,\n",
    "    add_start_index=True\n",
    ")\n",
    "\n",
    "# to actually split the documents we use the split_documetns method \n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"split documents into {len(all_splits)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68e6a49",
   "metadata": {},
   "source": [
    "Store\n",
    "\n",
    "We will use a vector db to store these chunks in order to index and embed them for search during runtime. We can do this in a single line to handle both embedding and chunk storage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ce6df91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4494c594-41b1-4c1f-9df3-3b32f1334205', '0d400d2f-1412-4965-b439-c8d42a534009', '29f00ce0-5d54-41e9-98ea-e58190de40d2']\n"
     ]
    }
   ],
   "source": [
    "documents_ids = vector_store.add_documents(documents=all_splits)\n",
    "print(documents_ids[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbb0214",
   "metadata": {},
   "source": [
    "Retrieval and Generation\n",
    "\n",
    "We will now create a simple application that takes a user question, retrieves the most relevant context, and inputs both the question and context to an LLM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da64f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: (answer goes here) \n",
      "Context: (context goes here) \n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "\n",
    "# pull prompt from hub. Good for maintaining prompts \n",
    "prompt = hub.pull('rlm/rag-prompt')\n",
    "\n",
    "example_message = prompt.invoke(\n",
    "    {\"context\": \"(context goes here)\", \"question\": \"(answer goes here)\"}\n",
    ").to_messages() # formats prompt output \n",
    "\n",
    "print(example_message[0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5686f21",
   "metadata": {},
   "source": [
    "We will now use LangGraph to tie in both retrieval and generation into a single application\n",
    "\n",
    "We need: \n",
    "\n",
    "1. State (our application)\n",
    "2. Nodes (the steps in our application)\n",
    "3. Control Flow (the ordering of steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d62aea",
   "metadata": {},
   "source": [
    "State\n",
    "\n",
    "State is what controls the input of our application, what is transferred between steps, and also whats output. \n",
    "\n",
    "For simple RAG we are tracking the question, answer, and context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c0b53716",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from typing_extensions import List, TypedDict\n",
    "\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e73e60",
   "metadata": {},
   "source": [
    "Nodes\n",
    "\n",
    "Nodes are our steps in the application. We will have retrieval and generation \n",
    "\n",
    "Retrieval uses similarity search to find documents most relevant to our question, generation will take both question and context to generate the most accurate answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "87bcac16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(state:State):\n",
    "    retrieved_docs = vector_store.similarity_search(state['question'])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "def generate(state:State):\n",
    "    # join docs content \n",
    "    docs_content = \"\".join([doc.page_content for doc in state[\"context\"]])\n",
    "    # pass question and context\n",
    "    messages = prompt.invoke(\n",
    "        {\"question\": state['question'], \"context\": docs_content}\n",
    "    )\n",
    "    # generate response \n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5792e87",
   "metadata": {},
   "source": [
    "Control Flow \n",
    "\n",
    "Here we compile our application into a single graph object connecting retrieval and generation into one step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7600700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langgraph.graph import START, StateGraph\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# create a sequence of steps: retrieve and generate \n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "# signify the start of the graph \n",
    "graph_builder.add_edge(START, 'retrieve')\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "75be663f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: [Document(id='9a86b340-837d-4570-9d3e-c8baa839eb73', metadata={'element_id': '911197208cf17cc409c7ff2273ddf78d', 'start_index': 0, 'source': 'data/ermentrout-and-terman-ch-1.pdf', 'page_number': 22, 'filename': 'ermentrout-and-terman-ch-1.pdf', 'category': 'Header', 'languages': '[\"eng\"]', 'filetype': 'application/pdf', 'coordinates': '{\"points\": [[269.16306294000003, 35.864399400000025], [269.16306294000003, 44.33259940000005], [385.2375271599998, 44.33259940000005], [385.2375271599998, 35.864399400000025]], \"system\": \"PixelSpace\", \"layout_width\": 439.0, \"layout_height\": 666.0}', 'file_directory': 'data', 'last_modified': '2025-09-16T12:28:07'}, page_content='1 The Hodgkin–Huxley Equations'), Document(id='64430e4c-657f-40e8-8e48-dcc52784c8e0', metadata={'element_id': 'ba194cde0dc093fbc9afb8b016df85de', 'filetype': 'application/pdf', 'category': 'Header', 'source': 'data/ermentrout-and-terman-ch-1.pdf', 'coordinates': '{\"points\": [[269.15882884, 35.864399400000025], [269.15882884, 44.33259940000005], [385.2392208000002, 44.33259940000005], [385.2392208000002, 35.864399400000025]], \"system\": \"PixelSpace\", \"layout_width\": 439.0, \"layout_height\": 666.0}', 'last_modified': '2025-09-16T12:28:07', 'filename': 'ermentrout-and-terman-ch-1.pdf', 'page_number': 4, 'languages': '[\"eng\"]', 'file_directory': 'data', 'start_index': 0}, page_content='1 The Hodgkin–Huxley Equations'), Document(id='679ebaa6-e87d-4e49-84e6-844661d000d2', metadata={'element_id': '6e01ba5b84c7cdb79ab9304595dc6287', 'source': 'data/ermentrout-and-terman-ch-1.pdf', 'filetype': 'application/pdf', 'file_directory': 'data', 'filename': 'ermentrout-and-terman-ch-1.pdf', 'last_modified': '2025-09-16T12:28:07', 'coordinates': '{\"points\": [[269.15882884, 35.864399400000025], [269.15882884, 44.33259940000005], [385.2392208000002, 44.33259940000005], [385.2392208000002, 35.864399400000025]], \"system\": \"PixelSpace\", \"layout_width\": 439.0, \"layout_height\": 666.0}', 'page_number': 6, 'category': 'Header', 'languages': '[\"eng\"]', 'start_index': 0}, page_content='1 The Hodgkin–Huxley Equations'), Document(id='4f3ea80b-344d-473f-88d4-8fecf31ed4ab', metadata={'filetype': 'application/pdf', 'category': 'Header', 'source': 'data/ermentrout-and-terman-ch-1.pdf', 'filename': 'ermentrout-and-terman-ch-1.pdf', 'coordinates': '{\"points\": [[269.16306294000003, 35.864399400000025], [269.16306294000003, 44.33259940000005], [385.2375271599998, 44.33259940000005], [385.2375271599998, 35.864399400000025]], \"system\": \"PixelSpace\", \"layout_width\": 439.0, \"layout_height\": 666.0}', 'languages': '[\"eng\"]', 'file_directory': 'data', 'page_number': 16, 'element_id': '7915a2fd723c17e6e121ced8f46c0091', 'start_index': 0, 'last_modified': '2025-09-16T12:28:07'}, page_content='1 The Hodgkin–Huxley Equations')]\n",
      "\n",
      "\n",
      "Answer: content=\"The Hodgkin–Huxley equations are a set of four coupled nonlinear differential equations that model how a neuron's membrane potential changes due to voltage-gated Na+ and K+ channels and a leak current. They include the membrane equation C_m dV/dt = I_ext − g_Na m^3 h (V − E_Na) − g_K n^4 (V − E_K) − g_L (V − E_L) and the gating dynamics dm/dt = α_m(V)(1 − m) − β_m(V)m, dn/dt = α_n(V)(1 − n) − β_n(V)n, dh/dt = α_h(V)(1 − h) − β_h(V)h with voltage-dependent rate functions α and β. They reproduce action potentials with specific conductances and reversal potentials.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 948, 'prompt_tokens': 111, 'total_tokens': 1059, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 768, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CGcmxtYpZ70qmOH5IyX2pvrIIxehh', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--4b3f57d3-6959-408a-915a-439f66ebf4ba-0' usage_metadata={'input_tokens': 111, 'output_tokens': 948, 'total_tokens': 1059, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 768}}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = graph.invoke({\"question\": \"What are the Hodgkin-Huxley equations?\"})\n",
    "\n",
    "print(f\"Context: {result[\"context\"]}\\n\\n\")\n",
    "print(f\"Answer: {result[\"answer\"]}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2857f88f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
